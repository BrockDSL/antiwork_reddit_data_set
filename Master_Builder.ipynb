{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Builder\n",
    "\n",
    "This notebook has the whole pipeline of build / analyze for the antiwork dataset.\n",
    "\n",
    "General components\n",
    "- 80,000 ids and general details\n",
    "- 80,00 pickle files of complete posts `/raw_data`\n",
    "\n",
    "General Steps\n",
    "- load up\n",
    "- create subset\n",
    "- apply different analysis measures\n",
    "\n",
    "\n",
    "Analysis Measures\n",
    "- automatic keyword generation\n",
    "- identified kewyord flagging\n",
    "- VADER Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/Users/tim/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "#Libraries and setup\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import praw\n",
    "import pickle\n",
    "import pprint\n",
    "import csv\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Avoid loading up all the pickles if there is no change there\n",
    "USE_PICKLES = False\n",
    "#Generated Keywords Section\n",
    "USE_KEYWORD_GEN = True\n",
    "\n",
    "\n",
    "#Seed for Sampling, to stay consistent\n",
    "RANDO_SEED = 1337\n",
    "\n",
    "\n",
    "BIG_DS_FILE = \"antiwork_massive.csv\"\n",
    "\n",
    "\n",
    "#How big to make the top and low dataframes\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "\n",
    "#Filenames for smaller subset datasets\n",
    "PLUS_FILENAME = \"antiwork_plus_top_quantile_1000.csv\"\n",
    "MID_FILENAME = \"antiwork_middle_quantile_1000.csv\"\n",
    "NEG_FILENAME = \"antiwork_neg_top_quantile_1000.csv\"\n",
    "\n",
    "\n",
    "#Entries that are very long that need to be truncated\n",
    "MAX_LENGTH_POST = 30000\n",
    "LONG_ENTRIES = [\n",
    "    \"qvne64\",\n",
    "    \"r61fv8\",\n",
    "    \"rmegty\"\n",
    "]\n",
    "\n",
    "\n",
    "NUM_TO_KEEP = 25\n",
    "GENERATED_KEYWORDS = \"keywords_1000_top_bottom_middle_quantile.csv\"\n",
    "\n",
    "#Some fixed keywords\n",
    "FIXED_KEYWORDS = [\n",
    "    \"strike\",\n",
    "    \"union\",\n",
    "    \"capitalism\",\n",
    "    \"socialism\",\n",
    "    \"anarchism\",\n",
    "    \"resignation\",\n",
    "    \"quit\",\n",
    "    \"abolition\",\n",
    "    \"asshole\",\n",
    "    \"labour|labor\",\n",
    "    \"scab\",\n",
    "    \"contract\",\n",
    "    \"temp\",\n",
    "    \"wage\",\n",
    "    \"fired\"\n",
    "]\n",
    "\n",
    "\n",
    "#PICKLE section\n",
    "\n",
    "PICKLE_PATH = \"/raw_data\"\n",
    "\n",
    "#Fields from PRAWL objects to keep during pickle load\n",
    "FIELDS_TO_KEEP = [\n",
    " #'all_awardings',\n",
    " #'allow_live_comments',\n",
    " #'approved_at_utc',\n",
    " #'approved_by',\n",
    " #'archived',\n",
    " 'author',\n",
    " #'author_flair_background_color',\n",
    " #'author_flair_css_class',\n",
    " #'author_flair_richtext',\n",
    " #'author_flair_template_id',\n",
    " #'author_flair_text',\n",
    " #'author_flair_text_color',\n",
    " #'author_flair_type',\n",
    " #'author_fullname',\n",
    " #'author_is_blocked',\n",
    " #'author_patreon_flair',\n",
    " #'author_premium',\n",
    " #'award',\n",
    " #'awarders',\n",
    " #'banned_at_utc',\n",
    " 'banned_by',\n",
    " #'can_gild',\n",
    " #'can_mod_post',\n",
    " #'category',\n",
    " #'clear_vote',\n",
    " #'clicked',\n",
    " #'comment_limit',\n",
    " #'comment_sort',\n",
    " #'comments',\n",
    " #'content_categories',\n",
    " #'contest_mode',\n",
    " #'created',\n",
    " 'created_utc',\n",
    " #'crosspost',\n",
    " #'delete',\n",
    " #'disable_inbox_replies',\n",
    " #'discussion_type',\n",
    " #'distinguished',\n",
    " #'domain',\n",
    " 'downs',\n",
    " #'downvote',\n",
    " #'duplicates',\n",
    " #'edit',\n",
    " #'edited',\n",
    " #'enable_inbox_replies',\n",
    " #'flair',\n",
    " #'fullname',\n",
    " #'gild',\n",
    " #'gilded',\n",
    " #'gildings',\n",
    " #'hidden',\n",
    " #'hide',\n",
    " #'hide_score',\n",
    " 'id',\n",
    " #'id_from_url',\n",
    " #'is_created_from_ads_ui',\n",
    " #'is_crosspostable',\n",
    " #'is_meta',\n",
    " #'is_original_content',\n",
    " #'is_reddit_media_domain',\n",
    " #'is_robot_indexable',\n",
    " #'is_self',\n",
    " #'is_video',\n",
    " 'likes',\n",
    " #'link_flair_background_color',\n",
    " #'link_flair_css_class',\n",
    " #'link_flair_richtext',\n",
    " #'link_flair_text',\n",
    " #'link_flair_text_color',\n",
    " #'link_flair_type',\n",
    " #'locked',\n",
    " #'mark_visited',\n",
    " #'media',\n",
    " #'media_embed',\n",
    " #'media_only',\n",
    " #'mod',\n",
    " #'mod_note',\n",
    " #'mod_reason_by',\n",
    " #'mod_reason_title',\n",
    " #'mod_reports',\n",
    " #'name',\n",
    " #'no_follow',\n",
    " #'num_comments',\n",
    " #'num_crossposts',\n",
    " #'num_duplicates',\n",
    " #'num_reports',\n",
    " #'over_18',\n",
    " #'parent_whitelist_status',\n",
    " #'parse',\n",
    " #'permalink',\n",
    " #'pinned',\n",
    " #'pwls',\n",
    " #'quarantine',\n",
    " 'removal_reason',\n",
    " #'removed_by',\n",
    " #'removed_by_category',\n",
    " #'reply',\n",
    " #'report',\n",
    " #'report_reasons',\n",
    " #'save',\n",
    " #'saved',\n",
    " 'score',\n",
    " #'secure_media',\n",
    " #'secure_media_embed',\n",
    " 'selftext',\n",
    " #'selftext_html',\n",
    " #'send_replies',\n",
    " 'shortlink',\n",
    " #'spoiler',\n",
    " #'stickied',\n",
    " #'subreddit',\n",
    " #'subreddit_id',\n",
    " #'subreddit_name_prefixed',\n",
    " #'subreddit_subscribers',\n",
    " #'subreddit_type',\n",
    " #'suggested_sort',\n",
    " #'thumbnail',\n",
    " #'thumbnail_height',\n",
    " #'thumbnail_width',\n",
    " 'title',\n",
    " #'top_awarded_type',\n",
    " #'total_awards_received',\n",
    " #'treatment_tags',\n",
    " #'unhide',\n",
    " #'unsave',\n",
    " 'ups',\n",
    " #'upvote',\n",
    " 'upvote_ratio',\n",
    " 'url',\n",
    " #'user_reports',\n",
    " 'view_count',\n",
    " #'visited',\n",
    " #'whitelist_status',\n",
    " #'wls'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Restrict Columns\n",
    "\n",
    "Opens all the pickles etc if needed\n",
    "\n",
    "**USE_PICKLES** will load from the harvested files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with assembling massive DF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_PICKLES == True:\n",
    "    \n",
    "    print(\"Loading Pickle Data\")\n",
    "    \n",
    "    #Whole Dataset assemble\n",
    "    path = os.getcwd() + PICKLE_PATH\n",
    "\n",
    "    total = []\n",
    "    problems = []\n",
    "    count = 0\n",
    "\n",
    "    for root,dirs,files in os.walk(path):\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            count+=1\n",
    "            if count % 5000 == 0:\n",
    "                print(count)\n",
    "\n",
    "            try:\n",
    "\n",
    "                sub = pickle.load(open(root+\"/\"+f,\"rb\"))\n",
    "                line = []\n",
    "\n",
    "                for attrib in FIELDS_TO_KEEP:\n",
    "                    entry = str(getattr(sub,attrib))\n",
    "                    line.append(entry)\n",
    "\n",
    "                total.append(line)\n",
    "\n",
    "            except:\n",
    "                problems.append(f)\n",
    "\n",
    "\n",
    "\n",
    "    ds_build = pd.DataFrame(total,columns=FIELDS_TO_KEEP)\n",
    "\n",
    "    #Convert Timestamps\n",
    "    dates = []\n",
    "    for row in ds_build.itertuples(index=False):\n",
    "      dates.append(pd.to_datetime(row.created_utc,unit='s'))\n",
    "\n",
    "    dates_df = pd.DataFrame(dates,columns = [\"timestamp\"])\n",
    "    ds_build = ds_build.join(dates_df)\n",
    "\n",
    "    \n",
    "    ds_build.to_csv(BIG_DS_FILE,index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    ds_build = pd.read_csv(BIG_DS_FILE)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('Done with assembling massive DF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict Rows\n",
    "\n",
    "- Currently items with `selftext` not null\n",
    "- Truncates down those really long column entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with dropping rows\n",
      "Done with truncating long entries\n"
     ]
    }
   ],
   "source": [
    "#Must have full_text\n",
    "# ie. post are text based\n",
    "\n",
    "ds_build = ds_build[ds_build[\"selftext\"].notnull()]\n",
    "ds_build.reset_index()\n",
    "\n",
    "print(\"Done with dropping rows\")\n",
    "\n",
    "\n",
    "#Enforce Max Size for known offenders\n",
    "\n",
    "for entry in LONG_ENTRIES:\n",
    "    ds_build.loc[ds_build[\"id\"] == entry,[\"selftext\"]] = ds_build[ds_build[\"id\"] == entry][\"selftext\"].str.slice(start=0,stop=MAX_LENGTH_POST)\n",
    "\n",
    "print(\"Done with truncating long entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do middle set? Quantile version\n",
    "\n",
    "- Sample out 1000 entries in Q2,Q3 using both `score` and `upvote_ratio`\n",
    "- Seed is set in configs cell at `RANDO_SEED` so we get consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mid_scoring = ds_build.query(\"(score >= score.quantile(0.25) and score <=score.quantile(0.75))\\\n",
    "#                and \\\n",
    "#                (upvote_ratio >= upvote_ratio.quantile(0.25) and upvote_ratio <= upvote_ratio.quantile(0.75))\")\\\n",
    "#            .sample(random_state=RANDO_SEED, n=SAMPLE_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Into top, middle & bottom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets Constructed!\n"
     ]
    }
   ],
   "source": [
    "#High Scores\n",
    "top_scoring = ds_build.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=False)[0:SAMPLE_SIZE]\n",
    "\n",
    "#Middle scores\n",
    "\n",
    "#Basic formulation\n",
    "#mid_scoring = ds_build.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=False)[40000:40000+SAMPLE_SIZE]\n",
    "\n",
    "#Quantile formulation\n",
    "mid_scoring = ds_build.query(\"(score >= score.quantile(0.25) and score <=score.quantile(0.75))\\\n",
    "                and \\\n",
    "                (upvote_ratio >= upvote_ratio.quantile(0.25) and upvote_ratio <= upvote_ratio.quantile(0.75))\")\\\n",
    "            .sample(random_state=RANDO_SEED, n=SAMPLE_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "#Low Scores\n",
    "low_scoring = ds_build.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=True)[0:SAMPLE_SIZE]\n",
    "\n",
    "print(\"Subsets Constructed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying VADER to top posts\n",
      "Applying VADER to middle posts\n",
      "Applying VADER to low posts\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"Applying VADER to top posts\")\n",
    "\n",
    "top_scoring[\"vscore_pos\"] = 0.0\n",
    "top_scoring[\"vscore_neg\"] = 0.0\n",
    "top_scoring[\"vscore_neu\"] = 0.0\n",
    "top_scoring[\"vscore_compound\"] = 0.0\n",
    "\n",
    "\n",
    "for index, row in top_scoring.iterrows():\n",
    "    ss = sid.polarity_scores(row[\"selftext\"])\n",
    "    top_scoring.at[index,'vscore_pos'] = float(ss[\"pos\"])\n",
    "    top_scoring.at[index,'vscore_neg'] = float(ss[\"neg\"])\n",
    "    top_scoring.at[index,'vscore_neu'] = float(ss[\"neu\"])\n",
    "    top_scoring.at[index,'vscore_compound'] = float(ss[\"compound\"])\n",
    "\n",
    "\n",
    "print(\"Applying VADER to middle posts\")\n",
    "\n",
    "mid_scoring[\"vscore_pos\"] = 0.0\n",
    "mid_scoring[\"vscore_neg\"] = 0.0\n",
    "mid_scoring[\"vscore_neu\"] = 0.0\n",
    "mid_scoring[\"vscore_compound\"] = 0.0\n",
    "\n",
    "\n",
    "for index, row in mid_scoring.iterrows():\n",
    "    ss = sid.polarity_scores(row[\"selftext\"])\n",
    "    mid_scoring.at[index,'vscore_pos'] = float(ss[\"pos\"])\n",
    "    mid_scoring.at[index,'vscore_neg'] = float(ss[\"neg\"])\n",
    "    mid_scoring.at[index,'vscore_neu'] = float(ss[\"neu\"])\n",
    "    mid_scoring.at[index,'vscore_compound'] = float(ss[\"compound\"])\n",
    "    \n",
    "print(\"Applying VADER to low posts\")\n",
    "\n",
    "low_scoring[\"vscore_pos\"] = 0.0\n",
    "low_scoring[\"vscore_neg\"] = 0.0\n",
    "low_scoring[\"vscore_neu\"] = 0.0\n",
    "low_scoring[\"vscore_compound\"] = 0.0\n",
    "\n",
    "\n",
    "for index, row in low_scoring.iterrows():\n",
    "    ss = sid.polarity_scores(row[\"selftext\"])\n",
    "    low_scoring.at[index,'vscore_pos'] = float(ss[\"pos\"])\n",
    "    low_scoring.at[index,'vscore_neg'] = float(ss[\"neg\"])\n",
    "    low_scoring.at[index,'vscore_neu'] = float(ss[\"neu\"])\n",
    "    low_scoring.at[index,'vscore_compound'] = float(ss[\"compound\"])\n",
    "    \n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Apply Identified Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appling Fixed Keyword Search\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Appling Fixed Keyword Search\")\n",
    "\n",
    "for kw in FIXED_KEYWORDS:\n",
    "    col_label = \"has_fixed_\"+kw.replace(\" \",\"_\")\n",
    "    \n",
    "    top_scoring[col_label] = top_scoring.selftext.str.contains(kw)\n",
    "    top_scoring[col_label] = top_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    mid_scoring[col_label] = mid_scoring.selftext.str.contains(kw)\n",
    "    mid_scoring[col_label] = mid_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    low_scoring[col_label] = low_scoring.selftext.str.contains(kw)\n",
    "    low_scoring[col_label] = low_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Generate Keywords\n",
    "\n",
    "**USE_KEYWORD_GEN** to generate those keywords from the list of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to manually add some stinker words\n",
    "stopwords_final = stopwords.words('english')\n",
    "stopwords_final.append('’ s')\n",
    "stopwords_final.append('’ m')\n",
    "stopwords_final.append('edit')\n",
    "stopwords_final.append('# x200b')\n",
    "stopwords_final.append('’ t')\n",
    "stopwords_final.append('’ ve')\n",
    "stopwords_final.append('’ re')\n",
    "stopwords_final.append('’ ll')\n",
    "stopwords_final.append('> >')\n",
    "stopwords_final.append('’ d')\n",
    "stopwords_final.append('[ https')\n",
    "stopwords_final.append('**\\ [')\n",
    "stopwords_final.append(\"ca n't\")\n",
    "stopwords_final.append('don ’ t')\n",
    "stopwords_final.append('didn ’ t')\n",
    "stopwords_final.append(\"wo n't\")\n",
    "stopwords_final.append('inc.**')\n",
    "stopwords_final.append(\"n't care\")\n",
    "stopwords_final.append('isn ’ t')\n",
    "stopwords_final.append('never')\n",
    "stopwords_final.append('everyone')\n",
    "stopwords_final.append('america')\n",
    "stopwords_final.append('gon na')\n",
    "stopwords_final.append('wasn ’ t')\n",
    "stopwords_final.append('doesn ’ t')\n",
    "stopwords_final.append('couldn ’ t')\n",
    "stopwords_final.append('wouldn ’ t')\n",
    "stopwords_final.append('haven ’ t')\n",
    "stopwords_final.append(\"n't need\")\n",
    "stopwords_final.append('hey')\n",
    "stopwords_final.append('im')\n",
    "stopwords_final.append('oh')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "#stopwords_final.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Automatic Keywords\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_KEYWORD_GEN == True:\n",
    "\n",
    "    keyword_freq = dict()\n",
    "\n",
    "    print(\"Generating Automatic Keywords\")\n",
    "    #Top\n",
    "    for index, row in top_scoring.iterrows():\n",
    "        tb = TextBlob(row[\"selftext\"])\n",
    "\n",
    "        for word in tb.noun_phrases:\n",
    "            if word.lower() not in stopwords_final:\n",
    "                if word in keyword_freq:\n",
    "                    keyword_freq[word] += 1\n",
    "                else:\n",
    "                    keyword_freq[word] = 1\n",
    "\n",
    "    #Middle\n",
    "    for index, row in mid_scoring.iterrows():\n",
    "        tb = TextBlob(row[\"selftext\"])\n",
    "\n",
    "        for word in tb.noun_phrases:\n",
    "            if word.lower() not in stopwords_final:\n",
    "                if word in keyword_freq:\n",
    "                    keyword_freq[word] += 1\n",
    "                else:\n",
    "                    keyword_freq[word] = 1\n",
    "                    \n",
    "                    \n",
    "    #Low\n",
    "    for index, row in low_scoring.iterrows():\n",
    "        tb = TextBlob(row[\"selftext\"])\n",
    "\n",
    "        for word in tb.noun_phrases:\n",
    "            if word.lower() not in stopwords_final:\n",
    "                if word in keyword_freq:\n",
    "                    keyword_freq[word] += 1\n",
    "                else:\n",
    "                    keyword_freq[word] = 1\n",
    "\n",
    "    with open(GENERATED_KEYWORDS,'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for w in sorted(keyword_freq, key=keyword_freq.get, reverse=True)[0:NUM_TO_KEEP]:\n",
    "            #print(w,\",\",keyword_freq[w])\n",
    "            writer.writerow([w,keyword_freq[w]])\n",
    "\n",
    "    gen_kw_df = pd.DataFrame.from_dict([keyword_freq])\n",
    "    gen_kw_df = gen_kw_df.T.reset_index(level=0)\n",
    "    gen_kw_df.columns = [\"keyword\",\"freq\"]\n",
    "    gen_kw_df = gen_kw_df.sort_values(by=\"freq\",ascending=False)[0:NUM_TO_KEEP]\n",
    "    gen_kw_df = gen_kw_df.reset_index()\n",
    "    del(gen_kw_df[\"index\"])\n",
    "\n",
    "else:\n",
    "    \n",
    "    gen_kw_df = pd.read_csv(GENERATED_KEYWORDS,header=None)\n",
    "    gen_kw_df.columns = [\"keyword\",\"freq\"]\n",
    "    \n",
    "    \n",
    "print(\"\\nDone!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fuck</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christmas</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>minimum wage</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thanks</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>new job</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>covid</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>update</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reddit</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ceo</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>full time</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amazon</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>anyway</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>please</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mcdonald</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pto</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sorry</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>black</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>walmart</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mental health</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kellogg</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>november</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>usa</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          keyword  freq\n",
       "0              hr   215\n",
       "1            well   145\n",
       "2            fuck   126\n",
       "3       christmas   125\n",
       "4           thank   122\n",
       "5    minimum wage   114\n",
       "6          thanks   110\n",
       "7         new job   106\n",
       "8           covid   103\n",
       "9          update    78\n",
       "10         reddit    66\n",
       "11            ceo    63\n",
       "12      full time    59\n",
       "13         amazon    56\n",
       "14         anyway    53\n",
       "15         please    51\n",
       "16       mcdonald    47\n",
       "17            pto    46\n",
       "18          sorry    46\n",
       "19          black    43\n",
       "20        walmart    40\n",
       "21  mental health    39\n",
       "22        kellogg    39\n",
       "23       november    39\n",
       "24            usa    36"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_kw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Apply Generated Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kw in gen_kw_df[\"keyword\"]:\n",
    "    \n",
    "    col_label = \"has_generated_\"+kw.replace(\" \",\"_\")\n",
    "    \n",
    "    top_scoring[col_label] = top_scoring.selftext.str.contains(kw)\n",
    "    top_scoring[col_label] = top_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    mid_scoring[col_label] = mid_scoring.selftext.str.contains(kw)\n",
    "    mid_scoring[col_label] = mid_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    low_scoring[col_label] = low_scoring.selftext.str.contains(kw)\n",
    "    low_scoring[col_label] = low_scoring[col_label].replace({False:0,True:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final sets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out Datasets to file\n",
    "\n",
    "top_scoring.to_csv(PLUS_FILENAME,index=False)\n",
    "mid_scoring.to_csv(MID_FILENAME, index=False)\n",
    "low_scoring.to_csv(NEG_FILENAME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
