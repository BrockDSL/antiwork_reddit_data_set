{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Builder\n",
    "\n",
    "This notebook has the whole pipeline of build / analyze for the antiwork dataset.\n",
    "\n",
    "General components\n",
    "- 80,000 ids and general details\n",
    "- 80,00 pickle files of complete posts `/raw_data`\n",
    "\n",
    "General Steps\n",
    "- load up\n",
    "- create subset\n",
    "- apply different analysis measures\n",
    "\n",
    "\n",
    "Analysis Measures\n",
    "- automatic keyword generation\n",
    "- identified kewyord flagging\n",
    "- VADER Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Libraries and setup\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import praw\n",
    "import pickle\n",
    "import pprint\n",
    "import csv\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Flags to skip steps\n",
    "#Avoid loading up all the pickles if there is no change there\n",
    "SKIP_PICKLES = True\n",
    "#Generated Keywords Section\n",
    "SKIP_KEYWORD_GEN = True\n",
    "\n",
    "BIG_DS_FILE = \"antiwork_massive.csv\"\n",
    "\n",
    "\n",
    "#How big to make the top and low dataframes\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "\n",
    "#Filenames for smaller subset datasets\n",
    "PLUS_FILENAME = \"antiwork_plus_top_1000.csv\"\n",
    "NEG_FILENAME = \"antiwork_neg_top_1000.csv\"\n",
    "\n",
    "\n",
    "#Entries that are very long that need to be truncated\n",
    "MAX_LENGTH_POST = 30000\n",
    "LONG_ENTRIES = [\n",
    "    \"qvne64\",\n",
    "    \"r61fv8\",\n",
    "    \"rmegty\"\n",
    "]\n",
    "\n",
    "\n",
    "NUM_TO_KEEP = 25\n",
    "GENERATED_KEYWORDS = \"keywords_1000_top_bottom.csv\"\n",
    "\n",
    "#Some fixed keywords\n",
    "FIXED_KEYWORDS = [\n",
    "    \"strike\",\n",
    "    \"union\",\n",
    "    \"capitalism\",\n",
    "    \"socialism\",\n",
    "    \"anarchism\",\n",
    "    \"resignation\",\n",
    "    \"quit\",\n",
    "    \"abolition\",\n",
    "    \"asshole\",\n",
    "    \"labour|labor\",\n",
    "    \"scab\",\n",
    "    \"contract\",\n",
    "    \"temp\",\n",
    "    \"wage\",\n",
    "    \"fired\"\n",
    "]\n",
    "\n",
    "\n",
    "#PICKLE section\n",
    "\n",
    "PICKLE_PATH = \"/raw_data\"\n",
    "\n",
    "#Fields from PRAWL objects to keep during pickle load\n",
    "FIELDS_TO_KEEP = [\n",
    " #'all_awardings',\n",
    " #'allow_live_comments',\n",
    " #'approved_at_utc',\n",
    " #'approved_by',\n",
    " #'archived',\n",
    " 'author',\n",
    " #'author_flair_background_color',\n",
    " #'author_flair_css_class',\n",
    " #'author_flair_richtext',\n",
    " #'author_flair_template_id',\n",
    " #'author_flair_text',\n",
    " #'author_flair_text_color',\n",
    " #'author_flair_type',\n",
    " #'author_fullname',\n",
    " #'author_is_blocked',\n",
    " #'author_patreon_flair',\n",
    " #'author_premium',\n",
    " #'award',\n",
    " #'awarders',\n",
    " #'banned_at_utc',\n",
    " 'banned_by',\n",
    " #'can_gild',\n",
    " #'can_mod_post',\n",
    " #'category',\n",
    " #'clear_vote',\n",
    " #'clicked',\n",
    " #'comment_limit',\n",
    " #'comment_sort',\n",
    " #'comments',\n",
    " #'content_categories',\n",
    " #'contest_mode',\n",
    " #'created',\n",
    " 'created_utc',\n",
    " #'crosspost',\n",
    " #'delete',\n",
    " #'disable_inbox_replies',\n",
    " #'discussion_type',\n",
    " #'distinguished',\n",
    " #'domain',\n",
    " 'downs',\n",
    " #'downvote',\n",
    " #'duplicates',\n",
    " #'edit',\n",
    " #'edited',\n",
    " #'enable_inbox_replies',\n",
    " #'flair',\n",
    " #'fullname',\n",
    " #'gild',\n",
    " #'gilded',\n",
    " #'gildings',\n",
    " #'hidden',\n",
    " #'hide',\n",
    " #'hide_score',\n",
    " 'id',\n",
    " #'id_from_url',\n",
    " #'is_created_from_ads_ui',\n",
    " #'is_crosspostable',\n",
    " #'is_meta',\n",
    " #'is_original_content',\n",
    " #'is_reddit_media_domain',\n",
    " #'is_robot_indexable',\n",
    " #'is_self',\n",
    " #'is_video',\n",
    " 'likes',\n",
    " #'link_flair_background_color',\n",
    " #'link_flair_css_class',\n",
    " #'link_flair_richtext',\n",
    " #'link_flair_text',\n",
    " #'link_flair_text_color',\n",
    " #'link_flair_type',\n",
    " #'locked',\n",
    " #'mark_visited',\n",
    " #'media',\n",
    " #'media_embed',\n",
    " #'media_only',\n",
    " #'mod',\n",
    " #'mod_note',\n",
    " #'mod_reason_by',\n",
    " #'mod_reason_title',\n",
    " #'mod_reports',\n",
    " #'name',\n",
    " #'no_follow',\n",
    " #'num_comments',\n",
    " #'num_crossposts',\n",
    " #'num_duplicates',\n",
    " #'num_reports',\n",
    " #'over_18',\n",
    " #'parent_whitelist_status',\n",
    " #'parse',\n",
    " #'permalink',\n",
    " #'pinned',\n",
    " #'pwls',\n",
    " #'quarantine',\n",
    " 'removal_reason',\n",
    " #'removed_by',\n",
    " #'removed_by_category',\n",
    " #'reply',\n",
    " #'report',\n",
    " #'report_reasons',\n",
    " #'save',\n",
    " #'saved',\n",
    " 'score',\n",
    " #'secure_media',\n",
    " #'secure_media_embed',\n",
    " 'selftext',\n",
    " #'selftext_html',\n",
    " #'send_replies',\n",
    " 'shortlink',\n",
    " #'spoiler',\n",
    " #'stickied',\n",
    " #'subreddit',\n",
    " #'subreddit_id',\n",
    " #'subreddit_name_prefixed',\n",
    " #'subreddit_subscribers',\n",
    " #'subreddit_type',\n",
    " #'suggested_sort',\n",
    " #'thumbnail',\n",
    " #'thumbnail_height',\n",
    " #'thumbnail_width',\n",
    " 'title',\n",
    " #'top_awarded_type',\n",
    " #'total_awards_received',\n",
    " #'treatment_tags',\n",
    " #'unhide',\n",
    " #'unsave',\n",
    " 'ups',\n",
    " #'upvote',\n",
    " 'upvote_ratio',\n",
    " 'url',\n",
    " #'user_reports',\n",
    " 'view_count',\n",
    " #'visited',\n",
    " #'whitelist_status',\n",
    " #'wls'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Restrict Columns\n",
    "\n",
    "Opens all the pickles etc if needed\n",
    "\n",
    "**SKIP_PICKLES** will rely on file contents instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with assembling massive DF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if SKIP_PICKLES == False:\n",
    "    \n",
    "    print(\"Loading Pickle Data\")\n",
    "    \n",
    "    #Whole Dataset assemble\n",
    "    path = os.getcwd() + PICKLE_PATH\n",
    "\n",
    "    total = []\n",
    "    problems = []\n",
    "    count = 0\n",
    "\n",
    "    for root,dirs,files in os.walk(path):\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            count+=1\n",
    "            if count % 5000 == 0:\n",
    "                print(count)\n",
    "\n",
    "            try:\n",
    "\n",
    "                sub = pickle.load(open(root+\"/\"+f,\"rb\"))\n",
    "                line = []\n",
    "\n",
    "                for attrib in FIELDS_TO_KEEP:\n",
    "                    entry = str(getattr(sub,attrib))\n",
    "                    line.append(entry)\n",
    "\n",
    "                total.append(line)\n",
    "\n",
    "            except:\n",
    "                problems.append(f)\n",
    "\n",
    "\n",
    "\n",
    "    ds_build = pd.DataFrame(total,columns=FIELDS_TO_KEEP)\n",
    "\n",
    "    #Convert Timestamps\n",
    "    dates = []\n",
    "    for row in ds_build.itertuples(index=False):\n",
    "      dates.append(pd.to_datetime(row.created_utc,unit='s'))\n",
    "\n",
    "    dates_df = pd.DataFrame(dates,columns = [\"timestamp\"])\n",
    "    ds_build = ds_build.join(dates_df)\n",
    "\n",
    "    \n",
    "    ds_build.to_csv(BIG_DS_FILE,index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    ds_build = pd.read_csv(BIG_DS_FILE)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('Done with assembling massive DF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict Rows\n",
    "\n",
    "- Currently items with `selftext` not null\n",
    "- Truncates down those really long column entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with dropping rows\n",
      "Done with truncating long entries\n"
     ]
    }
   ],
   "source": [
    "#Must have full_text\n",
    "# ie. post are text based\n",
    "\n",
    "ds_build = ds_build[ds_build[\"selftext\"].notnull()]\n",
    "ds_build.reset_index()\n",
    "\n",
    "print(\"Done with dropping rows\")\n",
    "\n",
    "\n",
    "#Enforce Max Size for known offenders\n",
    "\n",
    "for entry in LONG_ENTRIES:\n",
    "    ds_build.loc[ds_build[\"id\"] == entry,[\"selftext\"]] = ds_build[ds_build[\"id\"] == entry][\"selftext\"].str.slice(start=0,stop=MAX_LENGTH_POST)\n",
    "\n",
    "print(\"Done with truncating long entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Into top & bottom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets Constructed!\n"
     ]
    }
   ],
   "source": [
    "#antiwork_full = antiwork_full.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=False)[0:5000]\n",
    "\n",
    "#High Scores\n",
    "top_scoring = ds_build.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=False)[0:SAMPLE_SIZE]\n",
    "\n",
    "#Low Scores\n",
    "low_scoring = ds_build.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=True)[0:SAMPLE_SIZE]\n",
    "\n",
    "print(\"Subsets Constructed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying VADER to top posts\n",
      "Applying VADER to low posts\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"Applying VADER to top posts\")\n",
    "\n",
    "top_scoring[\"vscore_pos\"] = 0.0\n",
    "top_scoring[\"vscore_neg\"] = 0.0\n",
    "top_scoring[\"vscore_neu\"] = 0.0\n",
    "top_scoring[\"vscore_compound\"] = 0.0\n",
    "\n",
    "\n",
    "for index, row in top_scoring.iterrows():\n",
    "    ss = sid.polarity_scores(row[\"selftext\"])\n",
    "    top_scoring.at[index,'vscore_pos'] = float(ss[\"pos\"])\n",
    "    top_scoring.at[index,'vscore_neg'] = float(ss[\"neg\"])\n",
    "    top_scoring.at[index,'vscore_neu'] = float(ss[\"neu\"])\n",
    "    top_scoring.at[index,'vscore_compound'] = float(ss[\"compound\"])\n",
    "\n",
    "\n",
    "print(\"Applying VADER to low posts\")\n",
    "\n",
    "low_scoring[\"vscore_pos\"] = 0.0\n",
    "low_scoring[\"vscore_neg\"] = 0.0\n",
    "low_scoring[\"vscore_neu\"] = 0.0\n",
    "low_scoring[\"vscore_compound\"] = 0.0\n",
    "\n",
    "\n",
    "for index, row in low_scoring.iterrows():\n",
    "    ss = sid.polarity_scores(row[\"selftext\"])\n",
    "    low_scoring.at[index,'vscore_pos'] = float(ss[\"pos\"])\n",
    "    low_scoring.at[index,'vscore_neg'] = float(ss[\"neg\"])\n",
    "    low_scoring.at[index,'vscore_neu'] = float(ss[\"neu\"])\n",
    "    low_scoring.at[index,'vscore_compound'] = float(ss[\"compound\"])\n",
    "    \n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Apply Identified Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appling Fixed Keyword Search\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Appling Fixed Keyword Search\")\n",
    "\n",
    "for kw in FIXED_KEYWORDS:\n",
    "    col_label = \"has_fixed_\"+kw.replace(\" \",\"_\")\n",
    "    \n",
    "    top_scoring[col_label] = top_scoring.selftext.str.contains(kw)\n",
    "    top_scoring[col_label] = top_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    low_scoring[col_label] = low_scoring.selftext.str.contains(kw)\n",
    "    low_scoring[col_label] = low_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Generate Keywords\n",
    "\n",
    "**SKIP_KEYWORD_GEN** to forego and load from **GENERATED_KEYWORD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to manually add some stinker words\n",
    "stopwords_final = stopwords.words('english')\n",
    "stopwords_final.append('’ s')\n",
    "stopwords_final.append('’ m')\n",
    "stopwords_final.append('edit')\n",
    "stopwords_final.append('# x200b')\n",
    "stopwords_final.append('’ t')\n",
    "stopwords_final.append('’ ve')\n",
    "stopwords_final.append('’ re')\n",
    "stopwords_final.append('’ ll')\n",
    "stopwords_final.append('> >')\n",
    "stopwords_final.append('’ d')\n",
    "stopwords_final.append('[ https')\n",
    "stopwords_final.append('**\\ [')\n",
    "stopwords_final.append(\"ca n't\")\n",
    "stopwords_final.append('don ’ t')\n",
    "stopwords_final.append('didn ’ t')\n",
    "stopwords_final.append(\"wo n't\")\n",
    "stopwords_final.append('inc.**')\n",
    "stopwords_final.append(\"n't care\")\n",
    "stopwords_final.append('isn ’ t')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "#stopwords_final.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if SKIP_KEYWORD_GEN == False:\n",
    "\n",
    "    keyword_freq = dict()\n",
    "\n",
    "    print(\"Generating Automatic Keywords\")\n",
    "    #Top\n",
    "    for index, row in top_scoring.iterrows():\n",
    "        tb = TextBlob(row[\"selftext\"])\n",
    "\n",
    "        for word in tb.noun_phrases:\n",
    "            if word.lower() not in stopwords_final:\n",
    "                if word in keyword_freq:\n",
    "                    keyword_freq[word] += 1\n",
    "                else:\n",
    "                    keyword_freq[word] = 1\n",
    "\n",
    "    #Low\n",
    "    for index, row in low_scoring.iterrows():\n",
    "        tb = TextBlob(row[\"selftext\"])\n",
    "\n",
    "        for word in tb.noun_phrases:\n",
    "            if word.lower() not in stopwords_final:\n",
    "                if word in keyword_freq:\n",
    "                    keyword_freq[word] += 1\n",
    "                else:\n",
    "                    keyword_freq[word] = 1\n",
    "\n",
    "    with open(GENERATED_KEYWORDS,'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for w in sorted(keyword_freq, key=keyword_freq.get, reverse=True)[0:NUM_TO_KEEP]:\n",
    "            #print(w,\",\",keyword_freq[w])\n",
    "            writer.writerow([w,keyword_freq[w]])\n",
    "\n",
    "    gen_kw_df = pd.DataFrame.from_dict([keyword_freq])\n",
    "    gen_kw_df = gen_kw_df.T.reset_index(level=0)\n",
    "    gen_kw_df.columns = [\"keyword\",\"freq\"]\n",
    "    gen_kw_df = gen_kw_df.sort_values(by=\"freq\",ascending=False)[0:NUM_TO_KEEP]\n",
    "    gen_kw_df = gen_kw_df.reset_index()\n",
    "    del(gen_kw_df[\"index\"])\n",
    "\n",
    "else:\n",
    "    \n",
    "    gen_kw_df = pd.read_csv(GENERATED_KEYWORDS,header=None)\n",
    "    gen_kw_df.columns = [\"keyword\",\"freq\"]\n",
    "    \n",
    "    \n",
    "print(\"\\nDone!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hr</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minimum wage</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fuck</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>christmas</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>america</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thanks</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>black</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boss</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>oh</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>everyone</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>article</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>new job</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>reddit</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pto</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>please</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>never</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gon na</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>thank</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gm</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>usa</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sorry</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>iww</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>anyway</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>vp</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         keyword  freq\n",
       "0           well    32\n",
       "1             hr    32\n",
       "2   minimum wage    28\n",
       "3           fuck    24\n",
       "4      christmas    24\n",
       "5        america    24\n",
       "6         thanks    18\n",
       "7          black    17\n",
       "8           boss    16\n",
       "9             oh    15\n",
       "10      everyone    14\n",
       "11       article    14\n",
       "12       new job    13\n",
       "13        reddit    12\n",
       "14           pto    12\n",
       "15        please    12\n",
       "16         never    11\n",
       "17        gon na    11\n",
       "18         thank    11\n",
       "19            gm    11\n",
       "20           usa    11\n",
       "21         sorry    10\n",
       "22           iww    10\n",
       "23        anyway    10\n",
       "24            vp    10"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_kw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Apply Generated Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kw in gen_kw_df[\"keyword\"]:\n",
    "    \n",
    "    col_label = \"has_generated_\"+kw.replace(\" \",\"_\")\n",
    "    \n",
    "    top_scoring[col_label] = top_scoring.selftext.str.contains(kw)\n",
    "    top_scoring[col_label] = top_scoring[col_label].replace({False:0,True:1})\n",
    "    \n",
    "    low_scoring[col_label] = low_scoring.selftext.str.contains(kw)\n",
    "    low_scoring[col_label] = low_scoring[col_label].replace({False:0,True:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final sets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out Datasets to file\n",
    "\n",
    "top_scoring.to_csv(PLUS_FILENAME,index=False)\n",
    "low_scoring.to_csv(NEG_FILENAME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
