{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Builder\n",
    "\n",
    "This notebook has the whole pipeline of build / analyze for the antiwork dataset.\n",
    "\n",
    "General components\n",
    "- 80,000 ids and general details\n",
    "- 80,00 pickle files of complete posts `/raw_data`\n",
    "\n",
    "General Steps\n",
    "- load up and enrich\n",
    "- create subset\n",
    "- apply different analysis measures\n",
    "\n",
    "\n",
    "Analysis Measures\n",
    "- automatic keyword generation\n",
    "- identified kewyord flagging\n",
    "- VADER Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "- Load\n",
    "- Restrict\n",
    "- Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Libraries and setup\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import praw\n",
    "import pickle\n",
    "import pprint\n",
    "import csv\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flags and limits\n",
    "\n",
    "\n",
    "#Avoid loading up all the pickles if there is no change there\n",
    "SKIP_PICKLES = False\n",
    "\n",
    "BIG_DS_FILE = \"antiwork_massive.csv\"\n",
    "\n",
    "\n",
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "\n",
    "#Filenames for smaller subset datasets\n",
    "PLUS_FILENAME = \"antiwork_plus_top_1000.csv\"\n",
    "NEG_FILENAME = \"antiwork_neg_top_1000.csv\"\n",
    "\n",
    "\n",
    "#Entries that are very long that need to be truncated\n",
    "MAX_LENGTH_POST = 30000\n",
    "LONG_ENTRIES = [\n",
    "    \"qvne64\",\n",
    "    \"r61fv8\",\n",
    "    \"rmegty\"\n",
    "]\n",
    "\n",
    "#Generated Keywords Section\n",
    "SKIP_KEYWORD_GEN = False\n",
    "NUM_TO_KEEP = 25\n",
    "GENERATED_KEYWORDS = \"keywords_1000_top_bottom.csv\"\n",
    "\n",
    "\n",
    "#PICKLE section\n",
    "\n",
    "PICKLE_PATH = \"/raw_data\"\n",
    "\n",
    "#Fields from PRAWL objects to keep during pickle load\n",
    "FIELDS_TO_KEEP = [\n",
    " #'all_awardings',\n",
    " #'allow_live_comments',\n",
    " #'approved_at_utc',\n",
    " #'approved_by',\n",
    " #'archived',\n",
    " 'author',\n",
    " #'author_flair_background_color',\n",
    " #'author_flair_css_class',\n",
    " #'author_flair_richtext',\n",
    " #'author_flair_template_id',\n",
    " #'author_flair_text',\n",
    " #'author_flair_text_color',\n",
    " #'author_flair_type',\n",
    " #'author_fullname',\n",
    " #'author_is_blocked',\n",
    " #'author_patreon_flair',\n",
    " #'author_premium',\n",
    " #'award',\n",
    " #'awarders',\n",
    " #'banned_at_utc',\n",
    " 'banned_by',\n",
    " #'can_gild',\n",
    " #'can_mod_post',\n",
    " #'category',\n",
    " #'clear_vote',\n",
    " #'clicked',\n",
    " #'comment_limit',\n",
    " #'comment_sort',\n",
    " #'comments',\n",
    " #'content_categories',\n",
    " #'contest_mode',\n",
    " #'created',\n",
    " 'created_utc',\n",
    " #'crosspost',\n",
    " #'delete',\n",
    " #'disable_inbox_replies',\n",
    " #'discussion_type',\n",
    " #'distinguished',\n",
    " #'domain',\n",
    " 'downs',\n",
    " #'downvote',\n",
    " #'duplicates',\n",
    " #'edit',\n",
    " #'edited',\n",
    " #'enable_inbox_replies',\n",
    " #'flair',\n",
    " #'fullname',\n",
    " #'gild',\n",
    " #'gilded',\n",
    " #'gildings',\n",
    " #'hidden',\n",
    " #'hide',\n",
    " #'hide_score',\n",
    " 'id',\n",
    " #'id_from_url',\n",
    " #'is_created_from_ads_ui',\n",
    " #'is_crosspostable',\n",
    " #'is_meta',\n",
    " #'is_original_content',\n",
    " #'is_reddit_media_domain',\n",
    " #'is_robot_indexable',\n",
    " #'is_self',\n",
    " #'is_video',\n",
    " 'likes',\n",
    " #'link_flair_background_color',\n",
    " #'link_flair_css_class',\n",
    " #'link_flair_richtext',\n",
    " #'link_flair_text',\n",
    " #'link_flair_text_color',\n",
    " #'link_flair_type',\n",
    " #'locked',\n",
    " #'mark_visited',\n",
    " #'media',\n",
    " #'media_embed',\n",
    " #'media_only',\n",
    " #'mod',\n",
    " #'mod_note',\n",
    " #'mod_reason_by',\n",
    " #'mod_reason_title',\n",
    " #'mod_reports',\n",
    " #'name',\n",
    " #'no_follow',\n",
    " #'num_comments',\n",
    " #'num_crossposts',\n",
    " #'num_duplicates',\n",
    " #'num_reports',\n",
    " #'over_18',\n",
    " #'parent_whitelist_status',\n",
    " #'parse',\n",
    " #'permalink',\n",
    " #'pinned',\n",
    " #'pwls',\n",
    " #'quarantine',\n",
    " 'removal_reason',\n",
    " #'removed_by',\n",
    " #'removed_by_category',\n",
    " #'reply',\n",
    " #'report',\n",
    " #'report_reasons',\n",
    " #'save',\n",
    " #'saved',\n",
    " 'score',\n",
    " #'secure_media',\n",
    " #'secure_media_embed',\n",
    " 'selftext',\n",
    " #'selftext_html',\n",
    " #'send_replies',\n",
    " 'shortlink',\n",
    " #'spoiler',\n",
    " #'stickied',\n",
    " #'subreddit',\n",
    " #'subreddit_id',\n",
    " #'subreddit_name_prefixed',\n",
    " #'subreddit_subscribers',\n",
    " #'subreddit_type',\n",
    " #'suggested_sort',\n",
    " #'thumbnail',\n",
    " #'thumbnail_height',\n",
    " #'thumbnail_width',\n",
    " 'title',\n",
    " #'top_awarded_type',\n",
    " #'total_awards_received',\n",
    " #'treatment_tags',\n",
    " #'unhide',\n",
    " #'unsave',\n",
    " 'ups',\n",
    " #'upvote',\n",
    " 'upvote_ratio',\n",
    " 'url',\n",
    " #'user_reports',\n",
    " 'view_count',\n",
    " #'visited',\n",
    " #'whitelist_status',\n",
    " #'wls'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Restrict Columns\n",
    "\n",
    "Opens all the pickles etc if needed\n",
    "\n",
    "**SKIP_PICKLES** will rely on file contents instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pickle Data\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "Done with assembling massive DF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if SKIP_PICKLES == False:\n",
    "    \n",
    "    print(\"Loading Pickle Data\")\n",
    "    \n",
    "    #Whole Dataset assemble\n",
    "    path = os.getcwd() + PICKLE_PATH\n",
    "\n",
    "    total = []\n",
    "    problems = []\n",
    "    count = 0\n",
    "\n",
    "    for root,dirs,files in os.walk(path):\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            count+=1\n",
    "            if count % 5000 == 0:\n",
    "                print(count)\n",
    "\n",
    "            try:\n",
    "\n",
    "                sub = pickle.load(open(root+\"/\"+f,\"rb\"))\n",
    "                line = []\n",
    "\n",
    "                for attrib in FIELDS_TO_KEEP:\n",
    "                    entry = str(getattr(sub,attrib))\n",
    "                    line.append(entry)\n",
    "\n",
    "                total.append(line)\n",
    "\n",
    "            except:\n",
    "                problems.append(f)\n",
    "\n",
    "\n",
    "\n",
    "    ds_build = pd.DataFrame(total,columns=FIELDS_TO_KEEP)\n",
    "\n",
    "    #Convert Timestamps\n",
    "    dates = []\n",
    "    for row in ds_build.itertuples(index=False):\n",
    "      dates.append(pd.to_datetime(row.created_utc,unit='s'))\n",
    "\n",
    "    dates_df = pd.DataFrame(dates,columns = [\"timestamp\"])\n",
    "    ds_build = ds_build.join(dates_df)\n",
    "\n",
    "    \n",
    "    ds_build.to_csv(BIG_DS_FILE,index=False)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    ds_build = pd.read_csv(BIG_DS_FILE)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('Done with assembling massive DF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restrict Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with dropping rows\n",
      "Done with truncating long entries\n"
     ]
    }
   ],
   "source": [
    "#Must have full_text\n",
    "# ie. post are text based\n",
    "\n",
    "ds_build = ds_build[ds_build[\"selftext\"].notnull()]\n",
    "ds_build.reset_index()\n",
    "\n",
    "print(\"Done with dropping rows\")\n",
    "\n",
    "\n",
    "#Enforce Max Size for known offenders\n",
    "\n",
    "\n",
    "for entry in LONG_ENTRIES:\n",
    "    ds_build.loc[ds_build[\"id\"] == entry,[\"selftext\"]] = ds_build[ds_build[\"id\"] == entry][\"selftext\"].str.slice(start=0,stop=MAX_LENGTH_POST)\n",
    "\n",
    "print(\"Done with truncating long entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ds_build.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Into top & bottom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets Constructed!\n"
     ]
    }
   ],
   "source": [
    "#antiwork_full = antiwork_full.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=False)[0:5000]\n",
    "\n",
    "#High Scores\n",
    "top_scoring = ds_build.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=False)[0:SAMPLE_SIZE]\n",
    "\n",
    "#Low Scores\n",
    "low_scoring = ds_build.sort_values(by=[\"score\",\"upvote_ratio\"],ascending=True)[0:SAMPLE_SIZE]\n",
    "\n",
    "print(\"Subsets Constructed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying VADER to top posts\n",
      "Applying VADER to top posts\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"Applying VADER to top posts\")\n",
    "\n",
    "top_scoring[\"vscore_pos\"] = 0.0\n",
    "top_scoring[\"vscore_neg\"] = 0.0\n",
    "top_scoring[\"vscore_neu\"] = 0.0\n",
    "top_scoring[\"vscore_compound\"] = 0.0\n",
    "\n",
    "\n",
    "for index, row in top_scoring.iterrows():\n",
    "    ss = sid.polarity_scores(row[\"selftext\"])\n",
    "    top_scoring.at[index,'vscore_pos'] = float(ss[\"pos\"])\n",
    "    top_scoring.at[index,'vscore_neg'] = float(ss[\"neg\"])\n",
    "    top_scoring.at[index,'vscore_neu'] = float(ss[\"neu\"])\n",
    "    top_scoring.at[index,'vscore_compound'] = float(ss[\"compound\"])\n",
    "\n",
    "\n",
    "print(\"Applying VADER to top posts\")\n",
    "\n",
    "low_scoring[\"vscore_pos\"] = 0.0\n",
    "low_scoring[\"vscore_neg\"] = 0.0\n",
    "low_scoring[\"vscore_neu\"] = 0.0\n",
    "low_scoring[\"vscore_compound\"] = 0.0\n",
    "\n",
    "\n",
    "for index, row in low_scoring.iterrows():\n",
    "    ss = sid.polarity_scores(row[\"selftext\"])\n",
    "    low_scoring.at[index,'vscore_pos'] = float(ss[\"pos\"])\n",
    "    low_scoring.at[index,'vscore_neg'] = float(ss[\"neg\"])\n",
    "    low_scoring.at[index,'vscore_neu'] = float(ss[\"neu\"])\n",
    "    low_scoring.at[index,'vscore_compound'] = float(ss[\"compound\"])\n",
    "    \n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_scoring.sample(3)\n",
    "#low_scoring.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Apply Identified Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Posts\n",
      "Low Scoring\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#TOP Scoring\n",
    "\n",
    "print(\"Top Posts\")\n",
    "\n",
    "#does the post contain the word\n",
    "#- strike\n",
    "top_scoring['has_strike'] = top_scoring.selftext.str.contains(\"strike\")\n",
    "top_scoring['has_strike'] = top_scoring['has_strike'].replace({False:0,True:1})\n",
    "\n",
    "#- union\n",
    "top_scoring['has_union'] = top_scoring.selftext.str.contains(\"union\")\n",
    "top_scoring['has_union'] = top_scoring['has_union'].replace({False:0,True:1})\n",
    "\n",
    "#- capitalism\n",
    "top_scoring['has_capitalism'] = top_scoring.selftext.str.contains(\"capitalism\")\n",
    "top_scoring['has_capitalism'] = top_scoring['has_capitalism'].replace({False:0,True:1})\n",
    "\n",
    "#- socialism\n",
    "top_scoring['has_socialism'] = top_scoring.selftext.str.contains(\"socialism\")\n",
    "top_scoring['has_socialism'] = top_scoring['has_socialism'].replace({False:0,True:1})\n",
    "\n",
    "#- anarchism\n",
    "top_scoring['has_anarchism'] = top_scoring.selftext.str.contains(\"anarchism\")\n",
    "top_scoring['has_anarchism'] = top_scoring['has_anarchism'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#- resignation\n",
    "top_scoring['has_resignation'] = top_scoring.selftext.str.contains(\"resignation\")\n",
    "top_scoring['has_resignation'] = top_scoring['has_resignation'].replace({False:0,True:1})\n",
    "\n",
    "#- quit\n",
    "top_scoring['has_quit'] = top_scoring.selftext.str.contains(\"quit\")\n",
    "top_scoring['has_quit'] = top_scoring['has_quit'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#- abolition of work/wage labour\n",
    "top_scoring['has_abolition'] = top_scoring.selftext.str.contains(\"abolition\")\n",
    "top_scoring['has_abolition'] = top_scoring['has_abolition'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#- asshole boss / job\n",
    "top_scoring['has_asshole'] = top_scoring.selftext.str.contains(\"asshole\")\n",
    "top_scoring['has_asshole'] = top_scoring['has_asshole'].replace({False:0,True:1})\n",
    "\n",
    "#- labor\n",
    "top_scoring['has_labor'] = top_scoring.selftext.str.contains(\"labor\")\n",
    "top_scoring['has_labor'] = top_scoring['has_labor'].replace({False:0,True:1})\n",
    "\n",
    "#- scabs\n",
    "top_scoring['has_scabs'] = top_scoring.selftext.str.contains(\"scab\")\n",
    "top_scoring['has_scabs'] = top_scoring['has_scabs'].replace({False:0,True:1})\n",
    "\n",
    "#- contract\n",
    "top_scoring['has_contract'] = top_scoring.selftext.str.contains(\"contract\")\n",
    "top_scoring['has_contract'] = top_scoring['has_contract'].replace({False:0,True:1})\n",
    "\n",
    "#- temporary\n",
    "top_scoring['has_temporary'] = top_scoring.selftext.str.contains(\"temporary\")\n",
    "top_scoring['has_temporary'] = top_scoring['has_temporary'].replace({False:0,True:1})\n",
    "\n",
    "#- wage\n",
    "top_scoring['has_wage'] = top_scoring.selftext.str.contains(\"wage\")\n",
    "top_scoring['has_wage'] = top_scoring['has_wage'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#-fired\n",
    "top_scoring['has_fired'] = top_scoring.selftext.str.contains(\"fired\")\n",
    "top_scoring['has_fired'] = top_scoring['has_fired'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "print(\"Low Scoring\")\n",
    "#does the post contain the word\n",
    "#- strike\n",
    "low_scoring['has_strike'] = low_scoring.selftext.str.contains(\"strike\")\n",
    "low_scoring['has_strike'] = low_scoring['has_strike'].replace({False:0,True:1})\n",
    "\n",
    "#- union\n",
    "low_scoring['has_union'] = low_scoring.selftext.str.contains(\"union\")\n",
    "low_scoring['has_union'] = low_scoring['has_union'].replace({False:0,True:1})\n",
    "\n",
    "#- capitalism\n",
    "low_scoring['has_capitalism'] = low_scoring.selftext.str.contains(\"capitalism\")\n",
    "low_scoring['has_capitalism'] = low_scoring['has_capitalism'].replace({False:0,True:1})\n",
    "\n",
    "#- socialism\n",
    "low_scoring['has_socialism'] = low_scoring.selftext.str.contains(\"socialism\")\n",
    "low_scoring['has_socialism'] = low_scoring['has_socialism'].replace({False:0,True:1})\n",
    "\n",
    "#- anarchism\n",
    "low_scoring['has_anarchism'] = low_scoring.selftext.str.contains(\"anarchism\")\n",
    "low_scoring['has_anarchism'] = low_scoring['has_anarchism'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#- resignation\n",
    "low_scoring['has_resignation'] = low_scoring.selftext.str.contains(\"resignation\")\n",
    "low_scoring['has_resignation'] = low_scoring['has_resignation'].replace({False:0,True:1})\n",
    "\n",
    "#- quit\n",
    "low_scoring['has_quit'] = low_scoring.selftext.str.contains(\"quit\")\n",
    "low_scoring['has_quit'] = low_scoring['has_quit'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#- abolition of work/wage labour\n",
    "low_scoring['has_abolition'] = low_scoring.selftext.str.contains(\"abolition\")\n",
    "low_scoring['has_abolition'] = low_scoring['has_abolition'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#- asshole boss / job\n",
    "low_scoring['has_asshole'] = low_scoring.selftext.str.contains(\"asshole\")\n",
    "low_scoring['has_asshole'] = low_scoring['has_asshole'].replace({False:0,True:1})\n",
    "\n",
    "#- labor\n",
    "low_scoring['has_labor'] = low_scoring.selftext.str.contains(\"labor\")\n",
    "low_scoring['has_labor'] = low_scoring['has_labor'].replace({False:0,True:1})\n",
    "\n",
    "#- scabs\n",
    "low_scoring['has_scabs'] = low_scoring.selftext.str.contains(\"scab\")\n",
    "low_scoring['has_scabs'] = low_scoring['has_scabs'].replace({False:0,True:1})\n",
    "\n",
    "#- contract\n",
    "low_scoring['has_contract'] = low_scoring.selftext.str.contains(\"contract\")\n",
    "low_scoring['has_contract'] = low_scoring['has_contract'].replace({False:0,True:1})\n",
    "\n",
    "#- temporary\n",
    "low_scoring['has_temporary'] = low_scoring.selftext.str.contains(\"temporary\")\n",
    "low_scoring['has_temporary'] = low_scoring['has_temporary'].replace({False:0,True:1})\n",
    "\n",
    "#- wage\n",
    "low_scoring['has_wage'] = low_scoring.selftext.str.contains(\"wage\")\n",
    "low_scoring['has_wage'] = low_scoring['has_wage'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "#-fired\n",
    "low_scoring['has_fired'] = low_scoring.selftext.str.contains(\"fired\")\n",
    "low_scoring['has_fired'] = low_scoring['has_fired'].replace({False:0,True:1})\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#top_scoring.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Generate Keywords\n",
    "\n",
    "**SKIP_KEYWORD_GEN** to forego and load from **GENERATED_KEYWORD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to manually add some stinker words\n",
    "stopwords_final = stopwords.words('english')\n",
    "stopwords_final.append('’ s')\n",
    "stopwords_final.append('’ m')\n",
    "stopwords_final.append('edit')\n",
    "stopwords_final.append('# x200b')\n",
    "stopwords_final.append('’ t')\n",
    "stopwords_final.append('’ ve')\n",
    "stopwords_final.append('’ re')\n",
    "stopwords_final.append('’ ll')\n",
    "stopwords_final.append('> >')\n",
    "stopwords_final.append('’ d')\n",
    "stopwords_final.append('[ https')\n",
    "stopwords_final.append('**\\ [')\n",
    "stopwords_final.append(\"ca n't\")\n",
    "stopwords_final.append('don ’ t')\n",
    "stopwords_final.append('didn ’ t')\n",
    "stopwords_final.append(\"wo n't\")\n",
    "stopwords_final.append('inc.**')\n",
    "stopwords_final.append(\"n't care\")\n",
    "stopwords_final.append('isn ’ t')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "stopwords_final.append('')\n",
    "#stopwords_final.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "keyword_freq = dict()\n",
    "\n",
    "#Top\n",
    "for index, row in top_scoring.iterrows():\n",
    "    tb = TextBlob(row[\"selftext\"])\n",
    "\n",
    "    for word in tb.noun_phrases:\n",
    "        if word.lower() not in stopwords_final:\n",
    "            if word in keyword_freq:\n",
    "                keyword_freq[word] += 1\n",
    "            else:\n",
    "                keyword_freq[word] = 1\n",
    "\n",
    "#Low\n",
    "for index, row in low_scoring.iterrows():\n",
    "    tb = TextBlob(row[\"selftext\"])\n",
    "\n",
    "    for word in tb.noun_phrases:\n",
    "        if word.lower() not in stopwords_final:\n",
    "            if word in keyword_freq:\n",
    "                keyword_freq[word] += 1\n",
    "            else:\n",
    "                keyword_freq[word] = 1\n",
    "\n",
    "print(\"Keywords Found: \")\n",
    "with open(GENERATED_KEYWORDS,'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for w in sorted(keyword_freq, key=keyword_freq.get, reverse=True)[0:NUM_TO_KEEP]:\n",
    "        #print(w,\",\",keyword_freq[w])\n",
    "        writer.writerow([w,keyword_freq[w]])\n",
    "    \n",
    "print(\"\\nDone!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze - Apply Generated Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final sets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out Datasets to file\n",
    "\n",
    "top_scoring.to_csv(PLUS_FILENAME,index=False)\n",
    "low_scoring.to_csv(NEG_FILENAME, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
